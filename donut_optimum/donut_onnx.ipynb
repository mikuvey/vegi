{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import onnxruntime\n",
    "\n",
    "from transformers import DonutProcessor, AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForVision2Seq\n",
    "from datasets import load_dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EndUser\\anaconda3\\envs\\ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "# Load processor, tokenizer\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_options = onnxruntime.SessionOptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchFile",
     "evalue": "[ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from C:\\Users\\EndUser\\Desktop\\repos\\vegi\\donut_optimum\\models\\encoder_model.onnx failed:Load model C:\\Users\\EndUser\\Desktop\\repos\\vegi\\donut_optimum\\models\\encoder_model.onnx failed. File doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m      3\u001b[0m decoder_with_past_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mEndUser\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mrepos\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mvegi\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdonut_optimum\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdecoder_with_past_model.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Run on CPU\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# model = ORTModelForVision2Seq.from_pretrained(model_path, \u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#                                               export=False, \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#                                             #   provider_options = [{'config_file': config_file_path}]       \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#                                               )\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mORTModelForVision2Seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mdecoder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mdecoder_with_past_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_with_past_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCPUExecutionProvider\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcpu_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\EndUser\\anaconda3\\envs\\ai\\Lib\\site-packages\\optimum\\onnxruntime\\modeling_seq2seq.py:739\u001b[0m, in \u001b[0;36mORTModelForConditionalGeneration.load_model\u001b[1;34m(encoder_path, decoder_path, decoder_with_past_path, provider, session_options, provider_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\n\u001b[0;32m    711\u001b[0m     encoder_path: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    716\u001b[0m     provider_options: Optional[Dict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    717\u001b[0m ):\n\u001b[0;32m    718\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;124;03m    Creates an instance of [`~optimum.onnxruntime.modeling_seq2seq.ORTModelForConditionalGeneration`].\u001b[39;00m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;124;03m    Three inference sessions will be created for respectively the encoder, decoder and decoder with past key values\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;124;03m            for each provider: https://onnxruntime.ai/docs/api/c/group___global.html . Defaults to `None`.\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 739\u001b[0m     encoder_session \u001b[38;5;241m=\u001b[39m \u001b[43mORTModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    740\u001b[0m     decoder_session \u001b[38;5;241m=\u001b[39m ORTModel\u001b[38;5;241m.\u001b[39mload_model(decoder_path, provider, session_options, provider_options)\n\u001b[0;32m    742\u001b[0m     decoder_with_past_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\EndUser\\anaconda3\\envs\\ai\\Lib\\site-packages\\optimum\\onnxruntime\\modeling_ort.py:375\u001b[0m, in \u001b[0;36mORTModel.load_model\u001b[1;34m(path, provider, session_options, provider_options)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    373\u001b[0m     providers_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproviders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43msess_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproviders_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\EndUser\\anaconda3\\envs\\ai\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:419\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[1;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[0;32m    416\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[1;32mc:\\Users\\EndUser\\anaconda3\\envs\\ai\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:472\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[1;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_ep_custom_ops(session_options, providers, provider_options, available_providers)\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n\u001b[1;32m--> 472\u001b[0m     sess \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_config_from_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m     sess \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mInferenceSession(session_options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_config_from_model)\n",
      "\u001b[1;31mNoSuchFile\u001b[0m: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from C:\\Users\\EndUser\\Desktop\\repos\\vegi\\donut_optimum\\models\\encoder_model.onnx failed:Load model C:\\Users\\EndUser\\Desktop\\repos\\vegi\\donut_optimum\\models\\encoder_model.onnx failed. File doesn't exist"
     ]
    }
   ],
   "source": [
    "encoder_model_path = \"C:\\\\Users\\\\EndUser\\\\Desktop\\\\repos\\\\models\\\\encoder_model.onnx\"\n",
    "decoder_model_path = \"C:\\\\Users\\\\EndUser\\\\Desktop\\\\repos\\\\models\\\\decoder_model.onnx\"\n",
    "decoder_with_past_model_path = \"C:\\\\Users\\\\EndUser\\\\Desktop\\\\repos\\\\models\\\\decoder_with_past_model.onnx\"\n",
    "\n",
    "#Run on CPU\n",
    "# model = ORTModelForVision2Seq.from_pretrained(model_path, \n",
    "#                                               export=False, \n",
    "#                                               provider=\"CPUExecutionProvider\", \n",
    "#                                               session_options = cpu_options,\n",
    "#                                             #   provider_options = [{'config_file': config_file_path}]       \n",
    "#                                               )\n",
    "\n",
    "model = ORTModelForVision2Seq.load_model(encoder_path=encoder_model_path,\n",
    "                                         decoder_path=decoder_model_path,\n",
    "                                         decoder_with_past_path=decoder_with_past_model_path,\n",
    "                                         provider=\"CPUExecutionProvider\",\n",
    "                                         session_options = cpu_options,\n",
    "                                         provider_options = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
