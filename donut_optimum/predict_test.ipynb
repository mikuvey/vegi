{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import os\n",
    "from transformers import XLMRobertaTokenizerFast, MinLengthLogitsProcessor\n",
    "from transformers import DonutProcessor, AutoTokenizer\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import rotate, resize\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision import transforms\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"C:\\\\Users\\\\EndUser\\\\Desktop\\\\repos\\\\models\\\\\"\n",
    "providers=['CPUExecutionProvider']\n",
    "sess_options = onnxruntime.SessionOptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Onnx Inference\n",
    "\n",
    "encoder = onnxruntime.InferenceSession(os.path.join(model_folder, 'encoder_model.onnx'), sess_options,\n",
    "                                                    providers=providers)\n",
    "\n",
    "decoder = onnxruntime.InferenceSession(os.path.join(model_folder, 'decoder_model.onnx'), sess_options,\n",
    "                                                    providers=providers)\n",
    "\n",
    "decoder_with_past = onnxruntime.InferenceSession(os.path.join(model_folder, 'decoder_with_past_model.onnx'),\n",
    "                                                              sess_options, providers=providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EndUser\\anaconda3\\envs\\ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "# Load processor, tokenizer\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_folder, \"config.json\"), 'r') as f:\n",
    "            config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_name_or_path': 'naver-clova-ix/donut-base-finetuned-cord-v2', 'architectures': ['VisionEncoderDecoderModel'], 'decoder': {'_name_or_path': '', 'activation_dropout': 0.0, 'activation_function': 'gelu', 'add_cross_attention': True, 'add_final_layer_norm': True, 'architectures': None, 'attention_dropout': 0.0, 'bad_words_ids': None, 'begin_suppress_tokens': None, 'bos_token_id': 0, 'chunk_size_feed_forward': 0, 'classifier_dropout': 0.0, 'cross_attention_hidden_size': None, 'd_model': 1024, 'decoder_attention_heads': 16, 'decoder_ffn_dim': 4096, 'decoder_layerdrop': 0.0, 'decoder_layers': 4, 'decoder_start_token_id': None, 'diversity_penalty': 0.0, 'do_sample': False, 'dropout': 0.1, 'early_stopping': False, 'encoder_attention_heads': 16, 'encoder_ffn_dim': 4096, 'encoder_layerdrop': 0.0, 'encoder_layers': 12, 'encoder_no_repeat_ngram_size': 0, 'eos_token_id': 2, 'exponential_decay_length_penalty': None, 'finetuning_task': None, 'forced_bos_token_id': None, 'forced_eos_token_id': 2, 'id2label': {'0': 'LABEL_0', '1': 'LABEL_1'}, 'init_std': 0.02, 'is_decoder': True, 'is_encoder_decoder': False, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'length_penalty': 1.0, 'max_length': 20, 'max_position_embeddings': 768, 'min_length': 0, 'model_type': 'mbart', 'no_repeat_ngram_size': 0, 'num_beam_groups': 1, 'num_beams': 1, 'num_hidden_layers': 12, 'num_return_sequences': 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'pad_token_id': 1, 'prefix': None, 'problem_type': None, 'pruned_heads': {}, 'remove_invalid_values': False, 'repetition_penalty': 1.0, 'return_dict': True, 'return_dict_in_generate': False, 'scale_embedding': True, 'sep_token_id': None, 'suppress_tokens': None, 'task_specific_params': None, 'temperature': 1.0, 'tf_legacy_loss': False, 'tie_encoder_decoder': False, 'tie_word_embeddings': True, 'tokenizer_class': None, 'top_k': 50, 'top_p': 1.0, 'torch_dtype': None, 'torchscript': False, 'typical_p': 1.0, 'use_bfloat16': False, 'use_cache': True, 'vocab_size': 57580}, 'encoder': {'_name_or_path': '', 'add_cross_attention': False, 'architectures': None, 'attention_probs_dropout_prob': 0.0, 'bad_words_ids': None, 'begin_suppress_tokens': None, 'bos_token_id': None, 'chunk_size_feed_forward': 0, 'cross_attention_hidden_size': None, 'decoder_start_token_id': None, 'depths': [2, 2, 14, 2], 'diversity_penalty': 0.0, 'do_sample': False, 'drop_path_rate': 0.1, 'early_stopping': False, 'embed_dim': 128, 'encoder_no_repeat_ngram_size': 0, 'eos_token_id': None, 'exponential_decay_length_penalty': None, 'finetuning_task': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.0, 'hidden_size': 1024, 'id2label': {'0': 'LABEL_0', '1': 'LABEL_1'}, 'image_size': [1280, 960], 'initializer_range': 0.02, 'is_decoder': False, 'is_encoder_decoder': False, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'layer_norm_eps': 1e-05, 'length_penalty': 1.0, 'max_length': 20, 'min_length': 0, 'mlp_ratio': 4.0, 'model_type': 'donut-swin', 'no_repeat_ngram_size': 0, 'num_beam_groups': 1, 'num_beams': 1, 'num_channels': 3, 'num_heads': [4, 8, 16, 32], 'num_layers': 4, 'num_return_sequences': 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'pad_token_id': None, 'patch_size': 4, 'path_norm': True, 'prefix': None, 'problem_type': None, 'pruned_heads': {}, 'qkv_bias': True, 'remove_invalid_values': False, 'repetition_penalty': 1.0, 'return_dict': True, 'return_dict_in_generate': False, 'sep_token_id': None, 'suppress_tokens': None, 'task_specific_params': None, 'temperature': 1.0, 'tf_legacy_loss': False, 'tie_encoder_decoder': False, 'tie_word_embeddings': True, 'tokenizer_class': None, 'top_k': 50, 'top_p': 1.0, 'torch_dtype': None, 'torchscript': False, 'typical_p': 1.0, 'use_absolute_embeddings': False, 'use_bfloat16': False, 'window_size': 10}, 'is_encoder_decoder': True, 'model_type': 'vision-encoder-decoder', 'tie_word_embeddings': False, 'torch_dtype': 'float32', 'transformers_version': '4.40.2'}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#Max_length\n",
    "config_decoder = config['decoder']\n",
    "max_length = config_decoder['max_length']\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = tokenizer.pad_token_id\n",
    "eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(pad_token_id)\n",
    "print(eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Load document image\n",
    "dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n",
    "#image\n",
    "image = dataset[2][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_ids = processor(image, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': array([[[[-1., -1., -1., ..., -1., -1., -1.],\n",
      "         [-1., -1., -1., ..., -1., -1., -1.],\n",
      "         [-1., -1., -1., ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1., ..., -1., -1., -1.],\n",
      "         [-1., -1., -1., ..., -1., -1., -1.],\n",
      "         [-1., -1., -1., ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1., ..., -1., -1., -1.],\n",
      "         [-1., -1., -1., ..., -1., -1., -1.],\n",
      "         [-1., -1., -1., ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1., ..., -1., -1., -1.],\n",
      "         [-1., -1., -1., ..., -1., -1., -1.],\n",
      "         [-1., -1., -1., ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1., ..., -1., -1., -1.],\n",
      "         [-1., -1., -1., ..., -1., -1., -1.],\n",
      "         [-1., -1., -1., ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1., ..., -1., -1., -1.],\n",
      "         [-1., -1., -1., ..., -1., -1., -1.],\n",
      "         [-1., -1., -1., ..., -1., -1., -1.]]]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feed = {'pixel_values': encoder_input_ids['pixel_values']}  # Ensuring the input name matches the expected name in the model\n",
    "out_encoder = encoder.run(None, input_feed)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.5388077  -1.1014941   6.182827   ...  0.4769088   3.0601122\n",
      "   -1.711937  ]\n",
      "  [-0.3180547  -0.39086515  2.6406336  ... -0.07775511  0.3837932\n",
      "   -0.56481826]\n",
      "  [-0.0960921  -0.24702774  3.3573294  ... -0.15306045 -0.04358213\n",
      "   -0.11617677]\n",
      "  ...\n",
      "  [-1.1918149   0.21391855  1.8577919  ...  0.20430681 -0.21195805\n",
      "    1.2292104 ]\n",
      "  [ 0.02085814 -0.3813749   5.765299   ...  0.6419094   0.7135578\n",
      "   -0.3801818 ]\n",
      "  [-0.5709146  -0.93177253  9.305073   ...  0.84400254  2.6942708\n",
      "   -1.4047346 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(out_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[57579]]\n"
     ]
    }
   ],
   "source": [
    "#decoder Inputs\n",
    "task_prompt = \"<s_cord-v2>\"\n",
    "input_ids = tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"np\").input_ids.astype(dtype='int64') \n",
    "print(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_decoder = decoder.run(None, {'input_ids': decoder_input_ids, 'encoder_hidden_states': out_encoder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ()\n",
    "# keep track of which sequences are already finished\n",
    "unfinished_sequences = np.ones(1, dtype='int32')\n",
    "logits_processor = MinLengthLogitsProcessor(min_length=0, eos_token_id=eos_token_id)\n",
    "\n",
    "if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "eos_token_id_tensor = np.array(eos_token_id) if eos_token_id is not None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpast_key_value_input_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i): pkv \u001b[38;5;28;01mfor\u001b[39;00m i, pkv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(out_decoder[\u001b[38;5;241m1\u001b[39m:])}\n\u001b[0;32m     16\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m---> 18\u001b[0m next_tokens_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# argmax\u001b[39;00m\n\u001b[0;32m     20\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(next_tokens_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\EndUser\\anaconda3\\envs\\ai\\Lib\\site-packages\\transformers\\generation\\logits_process.py:154\u001b[0m, in \u001b[0;36mMinLengthLogitsProcessor.__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;129m@add_start_docstrings\u001b[39m(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m--> 154\u001b[0m     vocab_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n\u001b[0;32m    155\u001b[0m     eos_token_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos_token_id, device\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    156\u001b[0m     eos_token_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39misin(vocab_tensor, eos_token_id)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "past_key_values = None\n",
    "stop = False\n",
    "\n",
    "while not stop:\n",
    "    if past_key_values is None:\n",
    "        out_decoder = decoder.run(None, {'input_ids': input_ids, 'encoder_hidden_states': out_encoder})\n",
    "        logits = out_decoder[0]\n",
    "        past_key_values = {'past_key_value_input_' + str(k): out_decoder[k + 1] for k in\n",
    "                            range(len(out_decoder[1:]))}\n",
    "\n",
    "    else:\n",
    "        out_decoder = decoder_with_past.run(None, {'input_ids': input_ids[:, -1:],\n",
    "                                                                        **past_key_values})\n",
    "        logits = out_decoder[0]\n",
    "        past_key_values = {'past_key_value_input_' + str(i): pkv for i, pkv in enumerate(out_decoder[1:])}\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "\n",
    "    next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "    # argmax\n",
    "    next_tokens = np.argmax(next_tokens_scores, axis=-1).astype(dtype='int32')\n",
    "    scores += (next_tokens_scores,)\n",
    "\n",
    "    if eos_token_id is not None:\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "        next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "    # if eos_token was found in one sentence, set sentence to finished\n",
    "    if eos_token_id_tensor is not None:\n",
    "        unfinished_sequences = unfinished_sequences * (\n",
    "                np.tile(next_tokens, len(eos_token_id_tensor)) != np.prod(eos_token_id_tensor, axis=0))\n",
    "        # stop when each sentence is finished\n",
    "        if unfinished_sequences.max() == 0:\n",
    "            stop = True\n",
    "\n",
    "    if len(input_ids[0]) >= max_length:\n",
    "        stop = True\n",
    "\n",
    "    # update generated ids, model inputs, and length for next step\n",
    "    input_ids = np.concatenate([input_ids, next_tokens[:, None]], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
